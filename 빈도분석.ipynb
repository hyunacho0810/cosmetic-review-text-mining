{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOyaepbVls0kJfwcTRqadPC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"6aD3pfETEC2q","executionInfo":{"status":"error","timestamp":1714716973704,"user_tz":-540,"elapsed":357,"user":{"displayName":"HYUN A Cho","userId":"16526649522601318226"}},"outputId":"b565be38-6b31-4277-a034-38c2c3e46fe0"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name '_C' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-df543599c601>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mreplace_return_docstrings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 33\u001b[0;31m from .generic import (\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mContextManagers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mExplicitEnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_torch_pytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_model_output_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_pytree.Context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mraise\u001b[0m  \u001b[0;31m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0m__all__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"]}],"source":["import pandas as pd\n","from transformers import BertTokenizer\n","import matplotlib.pyplot as plt\n","import networkx as nx\n","from collections import Counter\n","from gensim.models import Word2Vec\n","import matplotlib.font_manager as fm\n","\n","# 한글 폰트 설정\n","plt.rcParams['font.family'] = 'NanumGothic'\n","\n","# 엑셀 파일 경로 설정\n","excel_file_path = '/content/에스트라 셀프 키워드 추출.xlsx'\n","\n","\n","# 엑셀 파일에서 데이터 로드\n","data = pd.read_excel(excel_file_path)\n","\n","\n","\n","# KoBERT 토크나이저 로드\n","tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n","\n","def preprocess_text(text):\n","    text = text.replace(\".\", \"\").replace(\",\", \"\").replace(\"!\", \"\").replace(\"?\", \"\").replace(\";\", \"\").replace(\":\", \"\").replace(\"ㅎ\", \"\").replace(\"ㅜ\", \"\").replace(\"ㅋ\", \"\").replace(\"~\", \"\")\n","    words = []\n","    current_word = \"\"\n","    for char in text:\n","        if char == \" \":\n","            if current_word:\n","                words.append(current_word)\n","                current_word = \"\"\n","        else:\n","            current_word += char\n","    if current_word:\n","        words.append(current_word)\n","    return words\n","\n","# 전처리된 데이터를 저장할 리스트 초기화\n","preprocessed_data = []\n","\n","# 각 행의 데이터를 전처리하여 리스트에 저장\n","for index, value in selected_column_data.items():\n","    input_text = str(value)\n","    preprocessed_text = preprocess_text(input_text)\n","    preprocessed_data.append(preprocessed_text)\n","\n","# preprocessed_data를 DataFrame으로 변환\n","df_preprocessed = pd.DataFrame(preprocessed_data)\n","\n","# 저장할 엑셀 파일 경로 설정\n","output_excel_path = r'C:\\Users\\LG\\OneDrive\\문서\\olive\\preprocessed_data.xlsx'\n","\n","# DataFrame을 엑셀 파일로 저장\n","df_preprocessed.to_excel(output_excel_path, index=False)\n","\n","print(\"preprocessed_data가 엑셀 파일로 저장되었습니다:\", output_excel_path)\n","\n","# 가장 빈도가 높은 단어 25개 추출\n","word_counts = Counter([token for sublist in preprocessed_data for token in sublist])\n","top_words = [word for word, _ in word_counts.most_common(30)]\n","\n","# Word2Vec 모델 학습\n","word2vec_model = Word2Vec(sentences=preprocessed_data, vector_size=100, window=5, min_count=1, workers=4)\n","\n","# 의미 연결망 분석을 위한 그래프 생성\n","graph = nx.Graph()\n","\n","# 유사도 점수를 계산하여 그래프에 엣지 추가\n","threshold = 0.1\n","for tokens in preprocessed_data:\n","    for i in range(len(tokens) - 1):\n","        word1 = tokens[i]\n","        word2 = tokens[i + 1]\n","        if word1 in top_words and word2 in top_words:\n","            similarity_score = word2vec_model.wv.similarity(word1, word2)\n","            if similarity_score > threshold:\n","                graph.add_edge(word1, word2, weight=similarity_score)\n","\n","print(\"그래프에 포함된 노드 수:\", len(graph.nodes()))\n","print(\"그래프에 포함된 엣지 수:\", len(graph.edges()))\n","\n","# 의미 연결망 시각화\n","plt.figure(figsize=(10, 6))\n","nx.draw(graph, with_labels=True, font_family='NanumGothic', font_size=10, node_size=1000, node_color='lightblue', edge_color='none')\n","plt.title('의미 연결망 분석')\n","plt.show()\n","\n","# 빈 노드와 연결된 엣지 제거\n","graph.remove_edges_from(nx.selfloop_edges(graph))\n","\n","# 노드가 하나도 연결되지 않은 경우 제거\n","isolated_nodes = list(nx.isolates(graph))\n","graph.remove_nodes_from(isolated_nodes)\n","\n","# 의미 연결망 시각화\n","plt.figure(figsize=(10, 6))\n","nx.draw(graph, with_labels=True, font_family='NanumGothic', font_size=10, node_size=1000, node_color='lightblue', edge_color='none')\n","plt.title('의미 연결망 분석')\n","plt.show()"]},{"cell_type":"code","source":["pip install --upgrade transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bEYtEHjWFfPa","executionInfo":{"status":"ok","timestamp":1714716823780,"user_tz":-540,"elapsed":15437,"user":{"displayName":"HYUN A Cho","userId":"16526649522601318226"}},"outputId":"87e869cb-4113-4af5-9aff-1e8eade8011e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"]}]},{"cell_type":"code","source":["pip uninstall torch torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZDj_4DyQGVRo","executionInfo":{"status":"ok","timestamp":1714717089527,"user_tz":-540,"elapsed":61784,"user":{"displayName":"HYUN A Cho","userId":"16526649522601318226"}},"outputId":"5035e1e3-48a0-480b-8cda-d22f995b66c8"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch 2.2.1+cu121\n","Uninstalling torch-2.2.1+cu121:\n","  Would remove:\n","    /usr/local/bin/convert-caffe2-to-onnx\n","    /usr/local/bin/convert-onnx-to-caffe2\n","    /usr/local/bin/torchrun\n","    /usr/local/lib/python3.10/dist-packages/functorch/*\n","    /usr/local/lib/python3.10/dist-packages/torch-2.2.1+cu121.dist-info/*\n","    /usr/local/lib/python3.10/dist-packages/torch/*\n","    /usr/local/lib/python3.10/dist-packages/torchgen/*\n","Proceed (Y/n)? \u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0mTraceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n","    status = run_func(*args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/uninstall.py\", line 105, in run\n","    uninstall_pathset = req.uninstall(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_install.py\", line 680, in uninstall\n","    uninstalled_pathset.remove(auto_confirm, verbose)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_uninstall.py\", line 375, in remove\n","    if auto_confirm or self._allowed_to_proceed(verbose):\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_uninstall.py\", line 415, in _allowed_to_proceed\n","    return ask(\"Proceed (Y/n)? \", (\"y\", \"n\", \"\")) != \"n\"\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/misc.py\", line 192, in ask\n","    response = input(message)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/pip3\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n","    return command.main(cmd_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n","    return self._main(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n","    return run(options, args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 207, in exc_logging_wrapper\n","    logger.debug(\"Exception information:\", exc_info=True)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1465, in debug\n","    self._log(DEBUG, msg, args, **kwargs)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n","    self.handle(record)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n","    self.callHandlers(record)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n","    hdlr.handle(record)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n","    self.emit(record)\n","  File \"/usr/lib/python3.10/logging/handlers.py\", line 75, in emit\n","    logging.FileHandler.emit(self, record)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1218, in emit\n","    StreamHandler.emit(self, record)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n","    msg = self.format(record)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n","    return fmt.format(record)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n","    formatted = super().format(record)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 686, in format\n","    record.exc_text = self.formatException(record.exc_info)\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 636, in formatException\n","    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n","  File \"/usr/lib/python3.10/traceback.py\", line 119, in print_exception\n","    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n","  File \"/usr/lib/python3.10/traceback.py\", line 502, in __init__\n","    self.stack = StackSummary.extract(\n","  File \"/usr/lib/python3.10/traceback.py\", line 383, in extract\n","    f.line\n","  File \"/usr/lib/python3.10/traceback.py\", line 306, in line\n","    self._line = linecache.getline(self.filename, self.lineno)\n","  File \"/usr/lib/python3.10/linecache.py\", line 30, in getline\n","    lines = getlines(filename, module_globals)\n","  File \"/usr/lib/python3.10/linecache.py\", line 46, in getlines\n","    return updatecache(filename, module_globals)\n","  File \"/usr/lib/python3.10/linecache.py\", line 136, in updatecache\n","    with tokenize.open(fullname) as fp:\n","  File \"/usr/lib/python3.10/tokenize.py\", line 396, in open\n","    encoding, lines = detect_encoding(buffer.readline)\n","  File \"/usr/lib/python3.10/tokenize.py\", line 365, in detect_encoding\n","    first = read_or_stop()\n","  File \"/usr/lib/python3.10/tokenize.py\", line 323, in read_or_stop\n","    return readline()\n","KeyboardInterrupt\n","^C\n"]}]},{"cell_type":"code","source":["pip install torch torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"drOZWImnFlhF","executionInfo":{"status":"ok","timestamp":1714716923046,"user_tz":-540,"elapsed":94590,"user":{"displayName":"HYUN A Cho","userId":"16526649522601318226"}},"outputId":"843e9d35-743e-47d6-b5a0-6f8de2090e4c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import BertTokenizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"id":"kQrpAWl7F94G","executionInfo":{"status":"error","timestamp":1714716978443,"user_tz":-540,"elapsed":342,"user":{"displayName":"HYUN A Cho","userId":"16526649522601318226"}},"outputId":"799c6e1d-1cb4-47ee-c286-b86fc7e5cfa4"},"execution_count":8,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name '_C' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-9d8153389f01>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mraise\u001b[0m  \u001b[0;31m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0m__all__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"]}]},{"cell_type":"code","source":["!pip install konlpy\n","!pip install pandas\n","\n","from konlpy.corpus import kobill\n","from konlpy.tag import Komoran\n","import nltk\n","from nltk.corpus import stopwords\n","import pandas as pd\n","\n","# KoNLPy를 사용하여 한글 불용어 리스트 생성\n","komoran = Komoran()\n","stopwords = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/korean_stopwords.txt\").values.tolist()\n","\n","# 불용어 제거 함수 정의\n","def remove_stopwords(tokens):\n","    return [token for token in tokens if token not in stopwords]\n","\n","# 예시 토큰 리스트\n","tokens_without_stopwords = [['안녕', '하세요', '한글', '텍스트', '입니다'], ['불용어', '제거', '연습', '중', '입니다']]\n","\n","# 불용어 제거 수행\n","tokens_without_stopwords = [remove_stopwords(tokens) for tokens in tokens_without_stopwords]\n","from konlpy.tag import Komoran\n","import re\n","\n","# KoNLPy를 사용하여 형태소 분석기 초기화\n","komoran = Komoran()\n","\n","# 엑셀 파일 불러오기\n","df = pd.read_excel('/content/에스트라 셀프 키워드 추출.xlsx')\n","\n","# 텍스트 정규화 및 피처 생성 함수 정의\n","def text_normalization(text):\n","    # 한글, 영문, 숫자를 제외한 모든 문자 제거\n","    text = re.sub(r\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9]\", \" \", text)\n","    # 형태소 분석 및 명사만 추출\n","    nouns = komoran.nouns(text)\n","    return nouns\n","\n","\n","# 전처리된 데이터에 대해 텍스트 정규화 및 피처 생성 수행\n","preprocessed_tokens = []\n","for index, row in df.iterrows():\n","    input_text = str(row)\n","    tokens = text_normalization(input_text)\n","    preprocessed_tokens.append(tokens)\n","\n","# 빈도분석을 위해 모든 토큰을 하나의 리스트로 펼치기\n","all_tokens = [token for sublist in preprocessed_tokens for token in sublist]\n","\n","# 빈도분석을 위해 NLTK의 FreqDist 사용\n","from nltk import FreqDist\n","\n","# 단어 빈도 계산\n","freq_dist = FreqDist(all_tokens)\n","\n","# 가장 많이 나오는 단어들 출력\n","top_words = freq_dist.most_common(10)\n","print(\"가장 많이 나오는 단어들:\", top_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"upnh0MZRGm8E","executionInfo":{"status":"ok","timestamp":1714717154865,"user_tz":-540,"elapsed":45558,"user":{"displayName":"HYUN A Cho","userId":"16526649522601318226"}},"outputId":"1cda551f-6f0c-49e9-f0e2-b2be29b4f337"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n","  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.25.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.0)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.5.0 konlpy-0.6.0\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","가장 많이 나오는 단어들: [('보습', 82), ('겨울', 74), ('지성', 69), ('순함', 10), ('자극', 8), ('여름', 7), ('번들', 7), ('건조', 6), ('진정', 6), ('나이트', 5)]\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from transformers import BertTokenizer\n","import matplotlib.pyplot as plt\n","import networkx as nx\n","from collections import Counter\n","from gensim.models import Word2Vec\n","import matplotlib.font_manager as fm\n","from google.colab import files  # Import the files module for Google Colab\n","\n","# 한글 폰트 설정\n","plt.rcParams['font.family'] = 'NanumGothic'\n","\n","# 엑셀 파일 업로드\n","uploaded = '/content/에스트라 셀프 키워드 추출.xlsx'  # Use files.upload() for Google Colab\n","\n","# 읽어올 열의 이름 설정\n","column_name = \"필드1\"\n","\n","# 엑셀 파일에서 데이터 로드\n","# Assume only one file is uploaded (get the first key)\n","file_name = list(uploaded.keys())[0]\n","data = pd.read_excel(file_name)\n","\n","# 선택한 열의 데이터만 추출\n","selected_column_data = data[column_name]\n","\n","# KoBERT 토크나이저 로드\n","tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n","\n","def preprocess_text(text):\n","    text = text.replace(\".\", \"\").replace(\",\", \"\").replace(\"!\", \"\").replace(\"?\", \"\").replace(\";\", \"\").replace(\":\", \"\").replace(\"ㅎ\", \"\").replace(\"ㅜ\", \"\").replace(\"ㅋ\", \"\").replace(\"~\", \"\")\n","    words = []\n","    current_word = \"\"\n","    for char in text:\n","        if char == \" \":\n","            if current_word:\n","                words.append(current_word)\n","                current_word = \"\"\n","        else:\n","            current_word += char\n","    if current_word:\n","        words.append(current_word)\n","    return words\n","\n","# 전처리된 데이터를 저장할 리스트 초기화\n","preprocessed_data = []\n","\n","# 각 행의 데이터를 전처리하여 리스트에 저장\n","for index, value in selected_column_data.items():\n","    input_text = str(value)\n","    preprocessed_text = preprocess_text(input_text)\n","    preprocessed_data.append(preprocessed_text)\n","\n","# preprocessed_data를 DataFrame으로 변환\n","df_preprocessed = pd.DataFrame(preprocessed_data)\n","\n","# DataFrame을 엑셀 파일로 저장\n","output_excel_path = 'preprocessed_data.xlsx'  # Use a relative path for saving\n","\n","# Save the DataFrame to Excel\n","df_preprocessed.to_excel(output_excel_path, index=False)\n","\n","print(\"preprocessed_data가 엑셀 파일로 저장되었습니다:\", output_excel_path)\n","\n","# 가장 빈도가 높은 단어 25개 추출\n","word_counts = Counter([token for sublist in preprocessed_data for token in sublist])\n","top_words = [word for word, _ in word_counts.most_common(30)]\n","\n","# Word2Vec 모델 학습\n","word2vec_model = Word2Vec(sentences=preprocessed_data, vector_size=100, window=5, min_count=1, workers=4)\n","\n","# 의미 연결망 분석을 위한 그래프 생성\n","graph = nx.Graph()\n","\n","# 유사도 점수를 계산하여 그래프에 엣지 추가\n","threshold = 0.1\n","for tokens in preprocessed_data:\n","    for i in range(len(tokens) - 1):\n","        word1 = tokens[i]\n","        word2 = tokens[i + 1]\n","        if word1 in top_words and word2 in top_words:\n","            similarity_score = word2vec_model.wv.similarity(word1, word2)\n","            if similarity_score > threshold:\n","                graph.add_edge(word1, word2, weight=similarity_score)\n","\n","print(\"그래프에 포함된 노드 수:\", len(graph.nodes()))\n","print(\"그래프에 포함된 엣지 수:\", len(graph.edges()))\n","\n","# 의미 연결망 시각화\n","plt.figure(figsize=(10, 6))\n","nx.draw(graph, with_labels=True, font_family='NanumGothic', font_size=10, node_size=1000, node_color='lightblue', edge_color='none')\n","plt.title('의미 연결망 분석')\n","plt.show()\n","\n","# 빈 노드와 연결된 엣지 제거\n","graph.remove_edges_from(nx.selfloop_edges(graph))\n","\n","# 노드가 하나도 연결되지 않은 경우 제거\n","isolated_nodes = list(nx.isolates(graph))\n","graph.remove_nodes_from(isolated_nodes)\n","\n","# 의미 연결망 시각화\n","plt.figure(figsize=(10, 6))\n","nx.draw(graph, with_labels=True, font_family='NanumGothic', font_size=10, node_size=1000, node_color='lightblue', edge_color='none')\n","plt.title('의미 연결망 분석')\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"ESah1pIDG3_K","executionInfo":{"status":"error","timestamp":1714718135424,"user_tz":-540,"elapsed":588,"user":{"displayName":"HYUN A Cho","userId":"16526649522601318226"}},"outputId":"f7717c77-5433-4c1b-fc84-e554a29f5fc3"},"execution_count":17,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name '_C' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-8428c2786efb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mreplace_return_docstrings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 33\u001b[0;31m from .generic import (\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mContextManagers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mExplicitEnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_torch_pytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_model_output_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_pytree.Context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mraise\u001b[0m  \u001b[0;31m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0m__all__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Tjq1mz4SJlKZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import networkx as nx\n","from collections import Counter\n","from gensim.models import Word2Vec\n","from transformers import BertTokenizer\n","from google.colab import files\n","\n","# Upload the Excel file\n","uploaded = files.upload()\n","\n","# Read the uploaded Excel file\n","file_name = list(uploaded.keys())[0]\n","data = pd.read_excel('/content/에스트라 셀프 키워드 추출.xlsx')\n","\n","# Define the column name containing text data\n","column_name = \"txt_inner\"\n","\n","# Extract text data from the specified column\n","selected_column_data = data[column_name]\n","\n","# Load KoBERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n","\n","def preprocess_text(text):\n","    text = text.replace(\".\", \"\").replace(\",\", \"\").replace(\"!\", \"\").replace(\"?\", \"\").replace(\";\", \"\").replace(\":\", \"\").replace(\"ㅎ\", \"\").replace(\"ㅜ\", \"\").replace(\"ㅋ\", \"\").replace(\"~\", \"\")\n","    words = text.split()\n","    return words\n","\n","# Preprocess the text data\n","preprocessed_data = [preprocess_text(str(value)) for value in selected_column_data]\n","\n","# Convert preprocessed data to DataFrame\n","df_preprocessed = pd.DataFrame(preprocessed_data)\n","\n","# Save preprocessed data to Excel\n","output_excel_path = 'preprocessed_data.xlsx'\n","df_preprocessed.to_excel(output_excel_path, index=False)\n","print(\"Preprocessed data saved to:\", output_excel_path)\n","\n","# Extract top frequent words\n","word_counts = Counter([token for sublist in preprocessed_data for token in sublist])\n","top_words = [word for word, _ in word_counts.most_common(30)]\n","\n","# Train Word2Vec model\n","word2vec_model = Word2Vec(sentences=preprocessed_data, vector_size=100, window=5, min_count=1, workers=4)\n","\n","# Build graph for semantic network analysis\n","graph = nx.Graph()\n","\n","# Add edges to the graph based on word similarity\n","threshold = 0.1\n","for tokens in preprocessed_data:\n","    for i in range(len(tokens) - 1):\n","        word1 = tokens[i]\n","        word2 = tokens[i + 1]\n","        if word1 in top_words and word2 in top_words:\n","            similarity_score = word2vec_model.wv.similarity(word1, word2)\n","            if similarity_score > threshold:\n","                graph.add_edge(word1, word2, weight=similarity_score)\n","\n","# Display semantic network visualization\n","plt.figure(figsize=(10, 6))\n","nx.draw(graph, with_labels=True, font_family='NanumGothic', font_size=10, node_size=1000, node_color='lightblue', edge_color='none')\n","plt.title('Semantic Network Analysis')\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"aGXwY2pSH5-3","executionInfo":{"status":"error","timestamp":1714717490850,"user_tz":-540,"elapsed":2280,"user":{"displayName":"HYUN A Cho","userId":"16526649522601318226"}},"outputId":"27fa4db8-7db7-403e-ce9e-fc2957b8c608"},"execution_count":13,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name '_C' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-ee28dadb8e6c>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mreplace_return_docstrings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 33\u001b[0;31m from .generic import (\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mContextManagers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mExplicitEnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_torch_pytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_model_output_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_pytree.Context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mraise\u001b[0m  \u001b[0;31m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0m__all__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"]}]},{"cell_type":"code","source":["from google.colab import drive\n","import pandas as pd\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Path to your data file in Google Drive\n","file_path = '/content/에스트라 셀프 키워드 추출.xlsx'\n","\n","# Read the Excel file\n","data = pd.read_excel(file_path)\n","\n","# Display the first few rows of the DataFrame\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":223},"id":"WVgot4cmJmYr","executionInfo":{"status":"ok","timestamp":1714718004288,"user_tz":-540,"elapsed":20102,"user":{"displayName":"HYUN A Cho","userId":"16526649522601318226"}},"outputId":"60553028-6a00-4cad-f05e-97c01d0a34b3"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"execute_result","data":{"text/plain":["      겨울    보습   지성 Unnamed: 3 Unnamed: 4 Unnamed: 5\n","0     촉촉    순함  속건조        NaN        NaN        NaN\n","1  피부염유발   NaN  NaN        NaN        NaN        NaN\n","2     순함  피부장벽  NaN        NaN        NaN        NaN\n","3    가벼움    순함  NaN        NaN        NaN        NaN\n","4     보습    꾸덕   촉촉        NaN        NaN        NaN"],"text/html":["\n","  <div id=\"df-c0aa28c8-799c-4303-a091-198fba2faab6\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>겨울</th>\n","      <th>보습</th>\n","      <th>지성</th>\n","      <th>Unnamed: 3</th>\n","      <th>Unnamed: 4</th>\n","      <th>Unnamed: 5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>촉촉</td>\n","      <td>순함</td>\n","      <td>속건조</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>피부염유발</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>순함</td>\n","      <td>피부장벽</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>가벼움</td>\n","      <td>순함</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>보습</td>\n","      <td>꾸덕</td>\n","      <td>촉촉</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0aa28c8-799c-4303-a091-198fba2faab6')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-c0aa28c8-799c-4303-a091-198fba2faab6 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-c0aa28c8-799c-4303-a091-198fba2faab6');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-176eea2d-ac4b-49dd-832a-11a2c421bd56\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-176eea2d-ac4b-49dd-832a-11a2c421bd56')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-176eea2d-ac4b-49dd-832a-11a2c421bd56 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"data","summary":"{\n  \"name\": \"data\",\n  \"rows\": 67,\n  \"fields\": [\n    {\n      \"column\": \"\\uaca8\\uc6b8\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 32,\n        \"samples\": [\n          \"\\ubb34\\ub09c\",\n          \"\\uc544\\ud1a0\\ud53c\\uc131\",\n          \"\\ub098\\uc774\\ud2b8\\ucf00\\uc5b4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ubcf4\\uc2b5\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"\\uac74\\uc131\",\n          \"\\ucfe8\\ub9c1\",\n          \"\\uc21c\\ud568\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\uc9c0\\uc131\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 14,\n        \"samples\": [\n          \"\\uc5ec\\ub984\\ubc88\\ub4e4\",\n          \"\\uc218\\ubd84\",\n          \"\\uc18d\\uac74\\uc870\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 3\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"\\uba54\\uc774\\ud06c\\uc5c5\\uc804\",\n          \"\\uc18d\\uac74\\uc870\",\n          \"\\uc21c\\ud568\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 4\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"\\uc131\\ubd84\",\n          \"\\uc5ec\\ub984\\ubc88\\ub4e4\",\n          \"\\ubb34\\uac70\\uc6c0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 5\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\\uc544\\uae30\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from konlpy.tag import Okt\n","\n","# Load your DataFrame or use the existing data DataFrame\n","# Assuming 'data' is already loaded with your Excel data\n","# If not, you can load it as follows:\n","# data = pd.read_excel('your_excel_file.xlsx')\n","\n","# Initialize Korean tokenizer (Okt from KoNLPy)\n","tokenizer = Okt()\n","\n","# Define a function to preprocess Korean text\n","def preprocess_text_korean(text):\n","    # Tokenize the text using Okt\n","    tokens = tokenizer.morphs(text)\n","\n","    # Join tokens back into a string\n","    processed_text = ' '.join(tokens)\n","\n","    return processed_text\n","\n","# Apply preprocessing to all text data in the DataFrame\n","data['processed_text'] = data['column_name'].apply(preprocess_text_korean)\n","\n","# Display the preprocessed text data\n","print(\"Preprocessed Text:\")\n","print(data['processed_text'].head())\n","\n","# Initialize TF-IDF Vectorizer for Korean text\n","tfidf_vectorizer = TfidfVectorizer()\n","\n","# Fit and transform the processed Korean text data\n","tfidf_matrix = tfidf_vectorizer.fit_transform(data['processed_text'])\n","\n","# Compute cosine similarity matrix\n","cosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)\n","\n","# Display the shape of the cosine similarity matrix\n","print(\"Cosine Similarity Matrix Shape:\", cosine_similarities.shape)\n","\n","# Display an example of cosine similarity (between first two documents)\n","print(\"Example Cosine Similarity (between first two documents):\", cosine_similarities[0, 1])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":512},"id":"WHmAji2sKHIS","executionInfo":{"status":"error","timestamp":1714718647997,"user_tz":-540,"elapsed":369,"user":{"displayName":"HYUN A Cho","userId":"16526649522601318226"}},"outputId":"a8ef22b5-3812-497c-dacb-6f66e95e85b6"},"execution_count":23,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'column_name'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'column_name'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-c2b11f88dcef>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Apply preprocessing to all text data in the DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'column_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text_korean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Display the preprocessed text data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3761\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3763\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'column_name'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"xB-OzixLM1U2"},"execution_count":null,"outputs":[]}]}